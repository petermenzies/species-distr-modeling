---
title: "Species Distribution Modeling"
author: "Peter Menzies"
date: "1/23/2022"
output:
  distill::distill_article:
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)

librarian::shelf(
  dismo, dplyr, DT, ggplot2, here, htmltools, leaflet, mapview, purrr, raster, readr, rgbif, rgdal, rJava, sdmpredictors, sf, spocc, tidyr, mgcv, GGally, caret, pdp, ranger, rpart, rpart.plot, rsample, skimr, vip, usdm)


# used geojsonio locally, but won't install on Taylor
#library(geojsonio)

select <- dplyr::select # overwrite raster::select

options(readr.show_col_types = F,
        DT.options = list(pageLength = 5))

# set random seed for reproducibility
set.seed(42)

# paths
dir_data      <- here("data/sdm")
pts_env_csv   <- file.path(dir_data, "pts_env.csv")
pts_geo       <- file.path(dir_data, "pts.geojson")
env_stack_grd <- file.path(dir_data, "env_stack.grd")
mdl_maxv_rds  <- file.path(dir_data, "mdl_maxent_vif.rds")

redo <- FALSE
```

## Overview

*Nomascus leucogenys*, commonly known as the southern yellow-cheeked gibbon, is a primate species native to southeast Asia. They display striking sexual dimorphism through their highly contrasted coloration in males and females---adult males being almost entirely black aside from their namesake yellow facial hair, and females being almost entirely golden-blonde. Unfortunately, like many other primates, this gibbon species is considered endangered. The following analysis attempts to model their distribution using open source, community science driven data and machine learning techniques.

And here are said gibbons in all their highly contrasted glory:

![yellow-cheeked gibbons](yellow_cheeked_gibbons.jpeg)

## Exploration

In this initial portion of the analysis we fetch observations from Global Biodiversity Information Facility (GBIF.org) using `spocc::occ` which interfaces with the GBIF API.

```{r}
obs_csv <- file.path(dir_data, "obs.csv")
obs_geo <- file.path(dir_data, "obs.geojson")

# get species occurrence data from GBIF with coordinates
if (!file.exists(obs_geo) | redo){
  (res <- spocc::occ(
    query = 'Nomascus gabriellae', 
    from = 'gbif', 
    has_coords = T,
    limit = 10000))

  df <- res$gbif$data[[1]] 
  nrow(df) # number of rows
  
  obs <- df %>% 
    sf::st_as_sf(
      coords = c("longitude", "latitude"),
      crs = st_crs(4326))
  
  readr::write_csv(df, obs_csv)
  
  geojson_write(obs, geometry = "point", file = obs_geo)
}

obs <- sf::read_sf(obs_geo)
nrow(obs) # number of rows
```

Removing "preserved specimens" (likely in museum collections) from the dataset.

**This was the only data cleaning/wrangling that I had to do**

```{r}
# Removing observation of a preserved specimen
obs <- obs %>% 
  filter(basisOfRecord != "PRESERVED_SPECIMEN")
```

Here we take a look at the spatial distribution of the queried observations.

**There are 235 observations for this species on GBIF**

```{r}
# show points on map
mapview::mapview(obs, map.types = "Stamen.Terrain")
```

Next, we view and assess a list of datasets containing potentially useful environmental data using `sdmppredictors`.

```{r}
dir_env <- file.path(dir_data, "env")

# set a default data directory
options(sdmpredictors_datadir = dir_env)

# choosing terrestrial
env_datasets <- sdmpredictors::list_datasets(terrestrial = TRUE, marine = FALSE)

# show table of datasets
env_datasets %>% 
  select(dataset_code, description, citation) %>% 
  DT::datatable()
```

Then we look at the layers within the datasets of interest.

```{r}
# choose datasets for a vector
env_datasets_vec <- c("WorldClim", "ENVIREM")

# get layers
env_layers <- sdmpredictors::list_layers(env_datasets_vec)
DT::datatable(env_layers)
```

And finally, after skimming the literature surrounding the species, we select the layers we think will best predict occurrence, and load them into a raster stack.

**This species is not thoroughly researched and there is still much to learn about their exact habitat preferences and the full extent of their range. However, it is known that these gibbons, unlike many of their relatives, prefer wet lowland forest---typically tropical evergreen, but have also been observed in deciduous forests and bamboo stands [New England Primate Conservancy](https://www.neprimateconservancy.org/yellow-cheeked-gibbon.html). Because of this uncertainty, I made sure to include elevation as parameter and from there selected layers that contained broadly-applicable environmental parameters.**

```{r}
# choose layers after some inspection and perhaps consulting literature
env_layers_vec <- c("WC_alt", "WC_bio1", "WC_bio2", "WC_bio6", "ER_tri", "ER_topoWet")

# get layers
env_stack <- load_layers(env_layers_vec)

# interactive plot layers, hiding all but first (select others)
plot(env_stack, nc = 2)
```

Here, we create a convex hull around the species observations. This will denote the range in which the modeling will take place. Because there are a high concentration of observations within a relatively small range, and the spatial resolution of the environmental rasterstack is limited, we're going to expand the range by buffering the convex hull---this will ultimately allow for some degree of randomness in our pseudo-absence points while having an equal amount of presence and absence points.

```{r}
obs_hull_geo <- file.path(dir_data, "obs_hull.geojson")
env_stack_grd <- file.path(dir_data, "env_stack.grd")

if (!file.exists(obs_hull_geo) | redo){
  # make convex hull around points of observation
  obs_hull <- sf::st_convex_hull(st_union(obs))
  
  # add buffer to allow more area for random absence points
  obs_hull_buffer <- st_buffer(obs_hull, 50000)
  
  # save obs hull
  write_sf(obs_hull_buffer, obs_hull_geo)
}
obs_hull <- read_sf(obs_hull_geo)

# show points on map
mapview(
  list(obs, obs_hull))
```

Then we'll crop the rasterstack to the range of the hull above.

```{r}
obs_hull_sp <- sf::as_Spatial(obs_hull)

# cropping env raster stack to extent of convex hull
env_stack <- raster::mask(env_stack, obs_hull_sp) %>%
  raster::crop(extent(obs_hull_sp))

writeRaster(env_stack, env_stack_grd, overwrite = TRUE)

mapview(obs) +
  mapview(env_stack, hide = T)
```

Taking a look at each of the raster layers.

```{r}
if (!file.exists(env_stack_grd) | redo){
  obs_hull_sp <- sf::as_Spatial(obs_hull)
  env_stack <- raster::mask(env_stack, obs_hull_sp) %>% 
    raster::crop(extent(obs_hull_sp))
  writeRaster(env_stack, env_stack_grd, overwrite=T)  
}
env_stack <- stack(env_stack_grd)

# show map
# mapview(obs) + 
#   mapview(env_stack, hide = T) # makes html too big for Github
plot(env_stack, nc = 2)
```

Next we create our random pseudo-absence points. First we create a raster that represents observation counts using the 'template' of one of the env_stack layers so that they have the same extent and resolution. Then we create a mask that has pixel values of 1 throughout the same extent except for pixels where observations were recorded. This mask provides the possible locations for the pseudo-absence points that we generate next using `dismo::randomPoints`---we create an amount equal to the number of true observations. Here they are plotted along with the observations:

```{r}
absence_geo <- file.path(dir_data, "absence.geojson")
pts_geo     <- file.path(dir_data, "pts.geojson")
pts_env_csv <- file.path(dir_data, "pts_env.csv")

if (!file.exists(absence_geo) | redo){
# creating raster of observation counts within the same pixels as the env_stack layers
  r_obs <- obs %>% 
    sf::as_Spatial() %>% 
    rasterize(y = env_stack[[1]], field = 1, fun = 'count')

  # create a mask representing locations where the species was NOT observed (inverse=TRUE) within the bounds of the raster stack
  # using the logical "> -Inf" gives us a raster for x that has values of 1 for all pixels from the first stack layer
  r_mask <- mask(x = env_stack[[1]] > -Inf, mask = r_obs, inverse = TRUE)
  
  # creating random pseudo-absence points within the mask we just created
  absence <- dismo::randomPoints(r_mask, nrow(obs)) %>% 
    as_tibble() %>% 
    st_as_sf(coords = c("x", "y"), crs = 4326)
  
  write_sf(absence, absence_geo, delete_dsn = TRUE)
}
absence <- read_sf(absence_geo)

mapview(obs, col.regions = "green") + 
  mapview(absence, col.regions = "gray")
```

The main purpose of everything up to this point was to create a single dataframe that contains all the environmental parameter values associated with each pixel in our range, and whether or not a yellow-cheeked gibbon was observed there. That's what we do here. First by creating a combined spatial df of observation and absence points, assigning a value of 1 to presence and 0 to absence---and then by extracting the values from all environmental layers associated with each observation and joining the two sets of data by their ID. As a result, we get this data table that will be the basis of all the modeling to come:

```{r}
if (!file.exists(pts_env_csv) | redo) {
  
  # create combined df of points with absence and presence represented by 0 and 1 respectively in "present" column
  pts <- rbind(
    obs %>% mutate(present = 1) %>% select(present),
    absence %>% mutate(present = 0)) %>% 
    mutate(ID = 1:n()) %>% relocate(ID)
  
  write_sf(pts, pts_geo)
  
  # extract all layer pixel values from `env_stack` where the points in `pts` occur
  pts_env <- raster::extract(env_stack, as_Spatial(pts), df=TRUE) %>% 
    tibble() %>% 
    # join present and geometry columns to raster value results for points
    left_join(
      pts %>% 
        select(ID, present),
      by = "ID") %>% 
    relocate(present, .after = ID) %>% 
    # extract lon, lat as single columns
    mutate(
      #present = factor(present),
      lon = st_coordinates(geometry)[,1],
      lat = st_coordinates(geometry)[,2]) %>% 
    select(-geometry)
  
  write_csv(pts_env, pts_env_csv)
}

pts_env <- read_csv(pts_env_csv)

pts_env %>% 
  # show first 10 presence, last 10 absence
  slice(c(1:10, (nrow(pts_env)-9):nrow(pts_env))) %>% 
  DT::datatable(
    rownames = F,
    options = list(
      dom = "t",
      pageLength = 20))
```

```{r}
pts_env %>% 
  select(-ID) %>% 
  mutate(
  present = factor(present)) %>% 
  pivot_longer(-present) %>% 
  ggplot() +
  geom_density(aes(x = value, fill = present)) + 
  scale_fill_manual(values = alpha(c("gray", "green"), 0.5)) +
  scale_x_continuous(expand=c(0,0)) +
  scale_y_continuous(expand=c(0,0)) +
  theme_bw() + 
  facet_wrap(~name, scales = "free") +
  theme(
    legend.position = c(1, 0),
    legend.justification = c(1, 0))
```


## Regression

In this part of the analysis we will employ several forms of linear regression as well as maximum entropy to estimate models for *Nomascus leucogenys* distribution. 

```{r}
datatable(pts_env, rownames = F)
```

Here we look for instances of correlation between our explanatory variables and remove those that might increase model error.

```{r}
GGally::ggpairs(
  select(pts_env, -ID),
  aes(color = factor(present)))
```

```{r}
# setup model data
d <- pts_env %>% 
  select(-ID, -"WC_alt", -"WC_bio6", -"ER_topoWet", -"ER_tri") %>%  # remove terms to reduce collinearity
  tidyr::drop_na()
nrow(d)
```

First we try multiple linear regression.

```{r}
# fit a linear model
mdl <- lm(present ~ ., data = d)
summary(mdl)
```

In order to use the predicted responses from this model, we need to transform the values such that they range from 0 to 1, and then we can predict presence based on which of the two values the output is closer to. We use the logit transformation as our "link function" in `glm()` to accomplish this.

```{r}
# showing that without a logistic transformation, the range of our predicted values is outside 0:1 and thus can't be used in their current state
y_predict <- predict(mdl, d, type="response")
y_true    <- d$present

range(y_predict)
```

```{r}
# and we need them to range from 0 to 1 as presence and absence are represented
range(y_true)
```

```{r}
mdl <- glm(present ~ ., family = binomial(link="logit"), data = d)
summary(mdl)
```

Success!

```{r}
# showing values are now between 0 and 1
y_predict <- predict(mdl, d, type="response")

range(y_predict)
```

Here we take a look at the individual relationships of each environmental parameter on presence.

```{r}
# show term plots
termplot(mdl, partial.resid = TRUE, se = TRUE, main = F, ylim="free")
```

Next we try out a generalized additive model with smooth predictors, and take a look at the termplots.

```{r}
# fit a generalized additive model with smooth predictors
mdl <- mgcv::gam(
  formula = present ~ s(WC_bio1) + 
    s(WC_bio2) + s(lon) + s(lat), 
  family = binomial(link = "logit"), data = d)
summary(mdl)
```

```{r}
plot(mdl, scale=0)
```
**The variables and value ranges that seem to contribute most to presence include `lat` between ~12.25 and 12.5~ degrees North, `WC_bio2` between ~8.0 and ~9.0 degrees C, and `WC_bio1` between ~21 and ~24 degrees C.**

And lastly, we employ one of the most widely used machine learning techniques for SDM: maximum entropy.

```{r}
librarian::shelf(
  maptools, sf)

mdl_maxent_rds <- file.path(dir_data, "mdl_maxent.rds")

# show version of maxent
if (!interactive())
  maxent()
```

```{r}
env_stack_grd <- file.path(dir_data, "env_stack.grd")
env_stack <- stack(env_stack_grd)
plot(env_stack, nc=2)
```
Here's shown variable contributions according to the maxent model and associated termplots: 

```{r}
# get presence-only observation points (maxent extracts raster values for you)
obs_geo <- file.path(dir_data, "obs.geojson")
obs_sp <- read_sf(obs_geo) %>% 
  sf::as_Spatial() # maxent prefers sp::SpatialPoints over newer sf::sf class

# fit a maximum entropy model
if (!file.exists(mdl_maxent_rds)){
  mdl <- maxent(env_stack, obs_sp)
  readr::write_rds(mdl, mdl_maxent_rds)
}
mdl <- read_rds(mdl_maxent_rds)

# plot variable contributions per predictor
plot(mdl)
```

```{r}
# plot term plots
response(mdl)
```

**The variables and value ranges that seem to contribute most to presence include `WC_alt` below ~500m, `WC_bio1` between ~20 and ~35 degrees C, and `WC_bio2` between ~10 and ~20 degrees C. This differs from the GAM results in part due to the fact that I removed `WC_alt` from my GAM becuase it was introducing collinearity and radically increasing the error. Maxent seemed to handle the collinear variables better and (as the literature suggested), altitude appears to have played an important role in distribution. `WC_bio1` and `WC_bio2` seem to have been represented similarly in both model termplots.**

And here are presence predictions from the maxent model:

```{r}
# predict
y_predict <- predict(env_stack, mdl) #, ext=ext, progress='')

plot(y_predict, main='Maxent, raw prediction')
data(wrld_simpl, package="maptools")
plot(wrld_simpl, add=TRUE, border='dark grey')
```


## Decision trees

In this part, we use decision trees to model presence, both in their singular form and in their aggregate form (random forest).

First step is to split dataset into training and testing sets with `rsample`.

```{r}
# create training set with 80% of full data
d_split  <- rsample::initial_split(d, prop = 0.8, strata = "present")
d_train  <- rsample::training(d_split) %>% 
  mutate(present = factor(present))

# show number of rows present is 0 vs 1
table(d$present)
```
```{r}
table(d_train$present)
```

```{r}
# run decision stump model
mdl <- rpart(
  present ~ ., data = d_train, 
  control = list(
    cp = 0, minbucket = 5, maxdepth = 1))
mdl
```

Testing the waters with a basic one split tree:

```{r}
# plot tree
par(mar = c(1, 1, 1, 1)) # setting plotting parameter for margin size
rpart.plot(mdl)
```

Now we a tree with default parameters, and take a look at model error as a function of the complexity parameter.

```{r}
# decision tree with defaults
mdl <- rpart(present ~ ., data = d_train)
mdl
```


```{r}
rpart.plot(mdl)

# plot complexity parameter (CP - minimum improvement in the model desired to qualify an additional split)
plotcp(mdl)

# rpart cross validation results
mdl$cptable
```
**Based on the complexity plot threshold, a tree of size 8 is recommended according to Brieman's 1-SE rule.**

we then cross validate using `caret` and look at variable importance and partial dependence using the results.

```{r}
# caret cross validation results
mdl_caret <- train(
  present ~ .,
  data       = d_train,
  method     = "rpart",
  trControl  = trainControl(method = "cv", number = 10),
  tuneLength = 20)

ggplot(mdl_caret)
```

```{r}
vip(mdl_caret, num_features = 40, bar = FALSE)
```

**Based on the rpart model, `lat`, `WC_bio1`, and `lon` are the top three most important variables.**

```{r}
# Construct partial dependence plots
p1 <- partial(mdl_caret, pred.var = "WC_bio1") %>% autoplot()
p2 <- partial(mdl_caret, pred.var = "lat") %>% autoplot()
p3 <- partial(mdl_caret, pred.var = c("WC_bio1", "lat")) %>% 
  plotPartial(levelplot = FALSE, zlab = "yhat", drape = TRUE, 
              colorkey = TRUE, screen = list(z = -20, x = -60))

# Display plots side by side
gridExtra::grid.arrange(p1, p2, p3, ncol = 3)
```
Here we move from a single tree to an ensemble of trees using the `ranger` random forest implementation. This can be a highly effective way to mitigate overfitting that can come about with decision trees.

First with default params:

```{r}
# number of features
n_features <- length(setdiff(names(d_train), "present"))

# fit a default random forest model
mdl_rf <- ranger(present ~ ., data = d_train)

# get out of the box RMSE
(default_rmse <- sqrt(mdl_rf$prediction.error))
```

Then with impurity-based and permutation-based variable importance:

```{r}
# re-run model with impurity-based variable importance
mdl_impurity <- ranger(
  present ~ ., data = d_train,
  importance = "impurity")

# re-run model with permutation-based variable importance
mdl_permutation <- ranger(
  present ~ ., data = d_train,
  importance = "permutation")
p1 <- vip::vip(mdl_impurity, bar = FALSE)
p2 <- vip::vip(mdl_permutation, bar = FALSE)

gridExtra::grid.arrange(p1, p2, nrow = 1)
```

**Our random forest model offers a slightly different picture of variable importance---according to the rf model, `lon` is more important than `WC-bio` (as opposed to the rpart model), and `WC_bio2` is shown to have greater importance than the rpart model suggested. Because random forests offer so much more variability in tree structures and allow for a degree of randomness to reveal otherwise overlooked importance, I would be much more inclined to trust these results over the single decision tree.**


## Evaluation

In this last stage we'll attempt to optimize our model by evaluating performance and calibrating parameters.

```{r}
# read points of observation: presence (1) and absence (0)
pts <- read_sf(pts_geo)

# read raster stack of environment
env_stack <- raster::stack(env_stack_grd)
```

Once again splitting the dataset 80/20 into training and testing sets.

```{r}
# create training set with 80% of full data
pts_split  <- rsample::initial_split(
  pts, prop = 0.8, strata = "present")
pts_train  <- rsample::training(pts_split)
pts_test   <- rsample::testing(pts_split)

pts_train_p <- pts_train %>% 
  filter(present == 1) %>% 
  as_Spatial()
pts_train_a <- pts_train %>% 
  filter(present == 0) %>% 
  as_Spatial()
```


```{r}
pairs(env_stack)
```

Next, we explore and try to mitigate some of the multicollinearity in our explanatory variables with techniques from the `usdm` package.

```{r}
# calculate variance inflation factor per predictor, a metric of multicollinearity between variables
vif(env_stack)
```


```{r}
# stepwise reduce predictors, based on a max correlation of 0.7 (max 1)
v <- vifcor(env_stack, th=0.7) 
v
```


```{r}
# reduce environmental raster stack by 
env_stack_v <- usdm::exclude(env_stack, v)

# show pairs plot after multicollinearity reduction with vifcor()
pairs(env_stack_v)
```

Now we can create a new rasterstack to feed into `maxent()` that excludes the variables with higher collinearity.

```{r}
# fit a maximum entropy model
if (!file.exists(mdl_maxv_rds)){
  mdl_maxv <- maxent(env_stack_v, sf::as_Spatial(pts_train))
  readr::write_rds(mdl_maxv, mdl_maxv_rds)
}
mdl_maxv <- read_rds(mdl_maxv_rds)

# plot variable contributions per predictor
plot(mdl_maxv)
```

**VIF collinearity removal excluded `ER_topowet`, `WC_bio6`, and `WC-alt`. The most important variables from greatest to lowest are now estimated as `WC_bio2`, `ER_tri`, and `WC_bio1`.


```{r}
# plot term plots
response(mdl_maxv)
```

We use `maxent` to create a new model using the refined explanatory variables.

```{r}
# predict
y_maxv <- predict(mdl_maxv, env_stack) #, ext=ext, progress='')

plot(y_maxv, main='Maxent, raw prediction')
data(wrld_simpl, package="maptools")
plot(wrld_simpl, add=TRUE, border='dark grey')
```

Then we evaluate the model using `dismo::evaluate()`

```{r}
pts_test_p <- pts_test %>% 
  filter(present == 1) %>% 
  as_Spatial()
pts_test_a <- pts_test %>% 
  filter(present == 0) %>% 
  as_Spatial()

y_maxv <- predict(mdl_maxv, env_stack)
#plot(y_maxv)

e <- dismo::evaluate(
  p     = pts_test_p,
  a     = pts_test_a, 
  model = mdl_maxv,
  x     = env_stack)
e
```

Then using the model evaluation object that's returned we can find a threshold value for assigning presence or absence. We do this with `dismo::threshold()`.

```{r}
thr <- threshold(e)[['spec_sens']]
thr
```

We test the estimated values from our latest model against the threshold value to determine which points the model and threshold deem as present and as absent. Then we construct a confusion matrix to help evaluate performance.

```{r}
# extract values from y_maxv raseter and locations in pts_test_p/a and compare them to thr value
p_true <- na.omit(raster::extract(y_maxv, pts_test_p) >= thr)
a_true <- na.omit(raster::extract(y_maxv, pts_test_a) < thr)

# (t)rue/(f)alse (p)ositive/(n)egative rates
tpr <- sum(p_true)/length(p_true)
fnr <- sum(!p_true)/length(p_true)
fpr <- sum(!a_true)/length(a_true)
tnr <- sum(a_true)/length(a_true)

matrix(
  c(tpr, fnr,
    fpr, tnr), 
  nrow=2, dimnames = list(
    c("present_obs", "absent_obs"),
    c("present_pred", "absent_pred")))
```

Last part of our evaluation will be to plot an ROC curve and a point representing the values from our threshold. 

```{r}
# add point to ROC plot
plot(e, 'ROC')

points(fpr, tpr, pch=23, bg="blue")
```

Based on the evaluation, this model predicts presence very well, although the false negative rate (~27%) is concerning---particularly when it comes to modeling that could inform management of endangered species. Moving forward I would most likely try a slightly lower threshold or adjust other model parameters. Here is our predicted distribution:

```{r}
plot(y_maxv > thr)
```







































